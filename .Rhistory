str(country_var[1])
str(country_var[2])
country_var <- rawData %>% select(contains(old_country_var)) %>% as.data.frame()
View(country_var)
str(country_var)
country_var <- rawData %>% select(contains(old_country_var)) %>% as.data.frame()
str(country_var)
typeof(country_var)
old_country_var <- survey_metadata %>% filter(survey==dataset) %>% select(country_var) %>% as.character()
country_var <- rawData %>% select(contains(old_country_var))
country_var[, old_country_var] <- gsub(".", "", country_var[, old_country_var], fixed=TRUE)
new_country_var <- country_var[, old_country_var]
View(new_country_var)
View(rawData)
rawData <- new_country_var %>% cbind(rawData)
View(rawData)
names(rawData)
names(rawData)[1]
names(rawData)[1] <- 'country'
View(rawData)
rawData <- rawData %>% arrange(country)
View(rawData)
earlyDropVars <- survey_metadata %>% filter(survey==dataset) %>% select(early_drop_vars) %>% str_split(" ") %>% unlist() %>% as.character()
finalDropVar <- survey_metadata %>% filter(survey==dataset) %>% select(final_drop_var) %>% as.character()
dropVars <- c(which(colnames(rawData) %in% earlyDropVars), grep(finalDropVar, names(rawData)):length(names(rawData)))
dropVars
subData <- rawData %>% select(-dropVars)
substantive_dat_vars <- ncol(subData)-1
View(subData)
missing_code <- survey_metadata %>% filter(survey==dataset) %>% select(missing_code) %>% as.numeric()
subData[(data.matrix(subData) - 5) <= missing_code]  <- NA
View(subData)
subData$country <- new_country_var
View(subData)
# Prepare data for percentmatch algorithm
# Andrew <andrew.flowers@fivethirtyeight.com>
setwd("~/survey-fraud/")
require(foreign)
require(readr)
require(dplyr)
require(stringr)
source("percentmatch.R")
# Load survey metadata file
survey_metadata <- read_csv("survey_metadata_for_cleaning.csv")
# Example: World Values Survey, Wave 1-6
data_files <- dir("./survey_data_files", full.names=TRUE)
rawData <- read.dta(file = data_files[6])
# NOTE: Will loop through ALL data_files to automate this process.
# Step 1: Record initial variable count, dataset name
orig_dat_vars <- ncol(rawData)
dataset <- str_extract(data_files[6], pattern='[^/]+$')
file_for_analysis <- gsub(".dta", "_temp.dta", dataset) # Note: do we need this?
# Step 2: Create clean country variable
old_country_var <- survey_metadata %>% filter(survey==dataset) %>% select(country_var) %>% as.character()
country_var <- rawData %>% select(contains(old_country_var))
country_var[, old_country_var] <- gsub(".", "", country_var[, old_country_var], fixed=TRUE)
new_country_var <- country_var[, old_country_var]
rawData <- new_country_var %>% cbind(rawData)
names(rawData)[1] <- 'country'
rawData <- rawData %>% arrange(country)
# Step 3: Drop unncessary variables (id, demographic, weight, and other metadata variables)
earlyDropVars <- survey_metadata %>% filter(survey==dataset) %>% select(early_drop_vars) %>% str_split(" ") %>% unlist() %>% as.character()
finalDropVar <- survey_metadata %>% filter(survey==dataset) %>% select(final_drop_var) %>% as.character()
dropVars <- c(which(colnames(rawData) %in% earlyDropVars), grep(finalDropVar, names(rawData)):length(names(rawData)))
subData <- rawData %>% select(-dropVars)
substantive_dat_vars <- ncol(subData)-1
# Step 4: Recode missing variables
missing_code <- survey_metadata %>% filter(survey==dataset) %>% select(missing_code) %>% as.numeric()
subData[(data.matrix(subData) - 5) <= missing_code]  <- NA # NOTE: The -5 calculation is a weird but necessary adjustment.
subData$country <- new_country_var # This is because the -5 calculation above makes Algeria (and potentially other countries) NA
# Step 5: Remove variables than have only one  unique non-missing value & variables with >10% missing data
varsToIgnore <- survey_metadata %>% filter(survey==dataset) %>% select(vars_to_ignore) %>% str_split(" ") %>% unlist() %>% as.character()
varsToInspect <- setdiff(names(subData), varsToIgnore)
countries <- levels(subData$country)
for (c in countries[1]){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Test printing
print(c)
print(head(pmatch))
}
countries[1]
# Prepare data for percentmatch algorithm
# Andrew <andrew.flowers@fivethirtyeight.com>
setwd("~/survey-fraud/")
require(foreign)
require(readr)
require(dplyr)
require(stringr)
source("percentmatch.R")
# Load survey metadata file
survey_metadata <- read_csv("survey_metadata_for_cleaning.csv")
# Example: World Values Survey, Wave 1-6
data_files <- dir("./survey_data_files", full.names=TRUE)
rawData <- read.dta(file = data_files[6])
# NOTE: Will loop through ALL data_files to automate this process.
# Step 1: Record initial variable count, dataset name
orig_dat_vars <- ncol(rawData)
dataset <- str_extract(data_files[6], pattern='[^/]+$')
file_for_analysis <- gsub(".dta", "_temp.dta", dataset) # Note: do we need this?
# Step 2: Create clean country variable
old_country_var <- survey_metadata %>% filter(survey==dataset) %>% select(country_var) %>% as.character()
country_var <- rawData %>% select(contains(old_country_var))
country_var[, old_country_var] <- gsub(".", "", country_var[, old_country_var], fixed=TRUE)
new_country_var <- country_var[, old_country_var]
rawData <- new_country_var %>% cbind(rawData)
names(rawData)[1] <- 'country'
rawData <- rawData %>% arrange(country)
# Step 3: Drop unncessary variables (id, demographic, weight, and other metadata variables)
earlyDropVars <- survey_metadata %>% filter(survey==dataset) %>% select(early_drop_vars) %>% str_split(" ") %>% unlist() %>% as.character()
finalDropVar <- survey_metadata %>% filter(survey==dataset) %>% select(final_drop_var) %>% as.character()
dropVars <- c(which(colnames(rawData) %in% earlyDropVars), grep(finalDropVar, names(rawData)):length(names(rawData)))
subData <- rawData %>% select(-dropVars)
substantive_dat_vars <- ncol(subData)-1
# Step 4: Recode missing variables
missing_code <- survey_metadata %>% filter(survey==dataset) %>% select(missing_code) %>% as.numeric()
subData[(data.matrix(subData) - 5) <= missing_code]  <- NA # NOTE: The -5 calculation is a weird but necessary adjustment.
subData$country <- new_country_var # This is because the -5 calculation above makes Algeria (and potentially other countries) NA
# Step 5: Remove variables than have only one  unique non-missing value & variables with >10% missing data
varsToIgnore <- survey_metadata %>% filter(survey==dataset) %>% select(vars_to_ignore) %>% str_split(" ") %>% unlist() %>% as.character()
varsToInspect <- setdiff(names(subData), varsToIgnore)
countries <- unique(subData$country)
countries
countries[1]
for (c in countries[1]){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Test printing
print(c)
print(head(pmatch))
}
source("percentmatch.R")
for (c in countries[1]){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Summary
print(c)
pmatchSummary(pmatch)
}
for (c in countries[2]){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Summary
print(c)
pmatchSummary(pmatch)
}
for (c in countries[3]){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Summary
print(c)
pmatchSummary(pmatch)
}
c
pmatchSummary(pmatch)
rbind(c, pmatchSummary(pmatch))
summary <- pmatchSummary(pmatch)
summary
str(pmatchSummary(pmatch))
dim(pmatchSummary(pmatch))
typeof(pmatchSummary(pmatch))
pmatchSummary <- function(pmatch){
print(paste0("100% matches: ", pmatch %>% filter(match==1) %>% tally()))
print(paste0("95% matches: ", pmatch %>% filter(match>.95) %>% tally()))
print(paste0("90% matches: ", pmatch %>% filter(match>.90) %>% tally()))
print(paste0("85% matches: ", pmatch %>% filter(match>.85) %>% tally()))
summaryVector <- c(
pmatch %>% filter(match==1) %>% tally(),
pmatch %>% filter(match>.95) %>% tally(),
pmatch %>% filter(match>.90) %>% tally(),
pmatch %>% filter(match>.85) %>% tally()
)
return(summaryVector)
}
source("percentmatch.R")
for (c in countries[3]){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Summary
print(c)
pmatchSummary(pmatch)
}
test <- pmatchSummary(pmatch)
test
rbind(c, test)
cbind(c, test)
for (c in countries){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Summary
print(c)
pmatchSummary(pmatch)
}
test
names(test)
pmatchSummary(pmatch, c)
c
source("percentmatch.R")
pmatch
pmatchSummary(pmatch, "Kazakhstan")
str(pmatchSummary(pmatch, "Kazakhstan"))
pmatchSummary <- function(pmatch, c){
print(paste0("100% matches: ", pmatch %>% filter(match==1) %>% tally()))
print(paste0("95% matches: ", pmatch %>% filter(match>.95) %>% tally()))
print(paste0("90% matches: ", pmatch %>% filter(match>.90) %>% tally()))
print(paste0("85% matches: ", pmatch %>% filter(match>.85) %>% tally()))
summaryVector <- data.frame(
country_id = c,
dup_observations_at_85 = pmatch %>% filter(match>=.85) %>% tally(),
dup_observations_at_90 = pmatch %>% filter(match>=.90) %>% tally(),
dup_observations_at_95 = pmatch %>% filter(match>=.95) %>% tally(),
dup_observations_at_100 = pmatch %>% filter(match==1) %>% tally()
)
return(summaryVector)
}
pmatchSummary(pmatch, "Kazakhstan")
pmatchSummary <- function(pmatch, c){
print(paste0("100% matches: ", pmatch %>% filter(match==1) %>% tally()))
print(paste0("95% matches: ", pmatch %>% filter(match>.95) %>% tally()))
print(paste0("90% matches: ", pmatch %>% filter(match>.90) %>% tally()))
print(paste0("85% matches: ", pmatch %>% filter(match>.85) %>% tally()))
summaryVector <- data.frame(
c,
pmatch %>% filter(match>=.85) %>% tally(),
pmatch %>% filter(match>=.90) %>% tally(),
pmatch %>% filter(match>=.95) %>% tally(),
pmatch %>% filter(match==1) %>% tally()
)
names(summaryVector) <- c("country_id", "dup_observations_at_85", "dup_observations_at_90", "dup_observations_at_95", "dup_observations_at_100")
return(summaryVector)
}
pmatchSummary(pmatch, "Kazakhstan")
test <- pmatchSummary(pmatch, "Kazakhstan")
test
summaryData <- data.frame()
rbind(test, test)
cbind(test, test)
# Prepare data for percentmatch algorithm
# Andrew <andrew.flowers@fivethirtyeight.com>
setwd("~/survey-fraud/")
require(foreign)
require(readr)
require(dplyr)
require(stringr)
source("percentmatch.R")
# Load survey metadata file
survey_metadata <- read_csv("survey_metadata_for_cleaning.csv")
# Example: World Values Survey, Wave 1-6
data_files <- dir("./survey_data_files", full.names=TRUE)
rawData <- read.dta(file = data_files[6])
# NOTE: Will loop through ALL data_files to automate this process.
# Step 1: Record initial variable count, dataset name
orig_dat_vars <- ncol(rawData)
dataset <- str_extract(data_files[6], pattern='[^/]+$')
file_for_analysis <- gsub(".dta", "_temp.dta", dataset) # Note: do we need this?
# Step 2: Create clean country variable
old_country_var <- survey_metadata %>% filter(survey==dataset) %>% select(country_var) %>% as.character()
country_var <- rawData %>% select(contains(old_country_var))
country_var[, old_country_var] <- gsub(".", "", country_var[, old_country_var], fixed=TRUE)
new_country_var <- country_var[, old_country_var]
rawData <- new_country_var %>% cbind(rawData)
names(rawData)[1] <- 'country'
rawData <- rawData %>% arrange(country)
# Step 3: Drop unncessary variables (id, demographic, weight, and other metadata variables)
earlyDropVars <- survey_metadata %>% filter(survey==dataset) %>% select(early_drop_vars) %>% str_split(" ") %>% unlist() %>% as.character()
finalDropVar <- survey_metadata %>% filter(survey==dataset) %>% select(final_drop_var) %>% as.character()
dropVars <- c(which(colnames(rawData) %in% earlyDropVars), grep(finalDropVar, names(rawData)):length(names(rawData)))
subData <- rawData %>% select(-dropVars)
substantive_dat_vars <- ncol(subData)-1
# Step 4: Recode missing variables
missing_code <- survey_metadata %>% filter(survey==dataset) %>% select(missing_code) %>% as.numeric()
subData[(data.matrix(subData) - 5) <= missing_code]  <- NA # NOTE: The -5 calculation is a weird but necessary adjustment.
subData$country <- new_country_var # This is because the -5 calculation above makes Algeria (and potentially other countries) NA
# Step 5: Remove variables than have only one  unique non-missing value & variables with >10% missing data
varsToIgnore <- survey_metadata %>% filter(survey==dataset) %>% select(vars_to_ignore) %>% str_split(" ") %>% unlist() %>% as.character()
varsToInspect <- setdiff(names(subData), varsToIgnore)
countries <- unique(subData$country)
summaryData <- data.frame()
for (c in countries){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Summary data, write to file
sumData <- pmatchSummary(pmatch, c)
print(sumData)
summaryData <- rbind(sumdata, summaryData)
}
for (c in countries){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Summary data, write to file
sumData <- pmatchSummary(pmatch, c)
print(sumData)
summaryData <- rbind(sumData, summaryData)
}
View(summaryData)
summaryData %>% arrange(desc(country_id)
summaryData %>% arrange(desc(country_id))
summaryData <- data.frame()
for (c in countries){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Summary data, write to file
sumData <- pmatchSummary(pmatch, c)
print(sumData) # Unnecessary
summaryData <- rbind(sumData, summaryData)
}
write_csv(summaryData %>% arrange(desc(country_id)), "replication_summary.csv")
data_files
setwd("~/survey-fraud/")
require(foreign)
require(readr)
require(dplyr)
require(stringr)
source("percentmatch.R")
# Load survey metadata file
survey_metadata <- read_csv("survey_metadata_for_cleaning.csv")
rawData <- read.dta(file = data_files[1])
# NOTE: Will loop through ALL data_files to automate this process.
# Step 1: Record initial variable count, dataset name
orig_dat_vars <- ncol(rawData)
dataset <- str_extract(data_files[1], pattern='[^/]+$')
file_for_analysis <- gsub(".dta", "_temp.dta", dataset)
data_files <- dir("./survey_data_files", full.names=TRUE)
rawData <- read.dta(file = data_files[1])
# NOTE: Will loop through ALL data_files to automate this process.
# Step 1: Record initial variable count, dataset name
orig_dat_vars <- ncol(rawData)
dataset <- str_extract(data_files[1], pattern='[^/]+$')
file_for_analysis <- gsub(".dta", "_temp.dta", dataset) # Note: do we need this?
View(rawData)
old_country_var <- survey_metadata %>% filter(survey==dataset) %>% select(country_var) %>% as.character()
country_var <- rawData %>% select(contains(old_country_var))
country_var[, old_country_var] <- gsub(".", "", country_var[, old_country_var], fixed=TRUE)
new_country_var <- country_var[, old_country_var]
rawData <- new_country_var %>% cbind(rawData)
names(rawData)[1] <- 'country'
rawData <- rawData %>% arrange(country)
View(rawData)
earlyDropVars <- survey_metadata %>% filter(survey==dataset) %>% select(early_drop_vars) %>% str_split(" ") %>% unlist() %>% as.character()
finalDropVar <- survey_metadata %>% filter(survey==dataset) %>% select(final_drop_var) %>% as.character()
dropVars <- c(which(colnames(rawData) %in% earlyDropVars), grep(finalDropVar, names(rawData)):length(names(rawData)))
subData <- rawData %>% select(-dropVars)
substantive_dat_vars <- ncol(subData)-1
dropVars <- c(which(colnames(rawData) %in% earlyDropVars), grep(finalDropVar, names(rawData)):length(names(rawData)))
earlyDropVars <- survey_metadata %>% filter(survey==dataset) %>% select(early_drop_vars) %>% str_split(" ") %>% unlist() %>% as.character()
finalDropVar <- survey_metadata %>% filter(survey==dataset) %>% select(final_drop_var) %>% as.character()
dropVars <- c(which(colnames(rawData) %in% earlyDropVars), grep(finalDropVar, names(rawData)):length(names(rawData)))
which(colnames(rawData) %in% earlyDropVars)
colnames(rawData) %in% earlyDropVars
grep(finalDropVar, names(rawData)):length(names(rawData))
grep(finalDropVar, names(rawData))
finalDropVar
View(survey_metadata)
names(rawData)
survey_metadata <- read_csv("survey_metadata_for_cleaning.csv")
earlyDropVars <- survey_metadata %>% filter(survey==dataset) %>% select(early_drop_vars) %>% str_split(" ") %>% unlist() %>% as.character()
finalDropVar <- survey_metadata %>% filter(survey==dataset) %>% select(final_drop_var) %>% as.character()
dropVars <- c(which(colnames(rawData) %in% earlyDropVars), grep(finalDropVar, names(rawData)):length(names(rawData)))
subData <- rawData %>% select(-dropVars)
substantive_dat_vars <- ncol(subData)-1
# Step 4: Recode missing variables
missing_code <- survey_metadata %>% filter(survey==dataset) %>% select(missing_code) %>% as.numeric()
subData[(data.matrix(subData) - 5) <= missing_code]  <- NA # NOTE: The -5 calculation is a weird but necessary adjustment.
subData$country <- new_country_var # This is because the -5 calculation above makes Algeria (and potentially other countries) NA
# Step 5: Remove variables than have only one  unique non-missing value & variables with >10% missing data
varsToIgnore <- survey_metadata %>% filter(survey==dataset) %>% select(vars_to_ignore) %>% str_split(" ") %>% unlist() %>% as.character()
varsToInspect <- setdiff(names(subData), varsToIgnore)
countries <- unique(subData$country)
summaryData <- data.frame()
countries
for (c in countries){
countryData <- subData %>% filter(country==c)
for (var in varsToInspect){
if ( ((sum(!is.na(countryData[,var]))/length(countryData[,var])) < 0.90) | (length(unique(as.character(countryData[,var]))) < 2)){
countryData[,var] <- NULL
}
}
# Step 6: Remove observations with >25% missing
countryData <- countryData[(rowSums(is.na(countryData))/ncol(countryData)) < 0.25,]
# Step 7: Send data to percentmatch algorithm
pmatch <- percentmatchR(countryData)
# Summary data, write to file
sumData <- pmatchSummary(pmatch, c)
print(sumData) # Unnecessary
summaryData <- rbind(sumData, summaryData)
}
View(subData)
subData %>% filter(country==c)
testData <- subData %>% filter(country==c)
View(testData)
setwd("~/survey-fraud/")
require(foreign)
require(readr)
require(dplyr)
require(stringr)
source("percentmatch.R")
# Load survey metadata file
survey_metadata <- read_csv("survey_metadata_for_cleaning.csv")
# Example: World Values Survey, Wave 1-6
data_files <- dir("./survey_data_files", full.names=TRUE)
rawData <- read.dta(file = data_files[2])
# NOTE: Will loop through ALL data_files to automate this process.
# Step 1: Record initial variable count, dataset name
orig_dat_vars <- ncol(rawData)
dataset <- str_extract(data_files[2], pattern='[^/]+$')
file_for_analysis <- gsub(".dta", "_temp.dta", dataset) # Note: do we need this?
# Step 2: Create clean country variable
old_country_var <- survey_metadata %>% filter(survey==dataset) %>% select(country_var) %>% as.character()
country_var <- rawData %>% select(contains(old_country_var))
country_var[, old_country_var] <- gsub(".", "", country_var[, old_country_var], fixed=TRUE)
new_country_var <- country_var[, old_country_var]
rawData <- new_country_var %>% cbind(rawData)
names(rawData)[1] <- 'country'
rawData <- rawData %>% arrange(country)
# Step 3: Drop unncessary variables (id, demographic, weight, and other metadata variables)
earlyDropVars <- survey_metadata %>% filter(survey==dataset) %>% select(early_drop_vars) %>% str_split(" ") %>% unlist() %>% as.character()
finalDropVar <- survey_metadata %>% filter(survey==dataset) %>% select(final_drop_var) %>% as.character()
dropVars <- c(which(colnames(rawData) %in% earlyDropVars), grep(finalDropVar, names(rawData)):length(names(rawData)))
subData <- rawData %>% select(-dropVars)
substantive_dat_vars <- ncol(subData)-1
# Step 4: Recode missing variables
missing_code <- survey_metadata %>% filter(survey==dataset) %>% select(missing_code) %>% as.numeric()
subData[(data.matrix(subData) - 5) <= missing_code]  <- NA # NOTE: The -5 calculation is a weird but necessary adjustment.
subData$country <- new_country_var # This is because the -5 calculation above makes Algeria (and potentially other countries) NA
# Step 5: Remove variables than have only one  unique non-missing value & variables with >10% missing data
varsToIgnore <- survey_metadata %>% filter(survey==dataset) %>% select(vars_to_ignore) %>% str_split(" ") %>% unlist() %>% as.character()
varsToInspect <- setdiff(names(subData), varsToIgnore)
countries <- unique(subData$country)
summaryData <- data.frame()
View(country_var)
country_var <- rawData %>% select(contains(old_country_var))
View(country_var)
